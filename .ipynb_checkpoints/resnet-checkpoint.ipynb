{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58eda99f-9f7c-4073-b8e9-134f187874dc",
   "metadata": {
    "id": "58eda99f-9f7c-4073-b8e9-134f187874dc"
   },
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fd4596-3af6-4df1-9ff0-ec0f1240e114",
   "metadata": {
    "id": "41fd4596-3af6-4df1-9ff0-ec0f1240e114"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def load_mnist_dataset():\n",
    "\n",
    "  # load data from tensorflow framework\n",
    "  ((trainData, trainLabels), (testData, testLabels)) = mnist.load_data() \n",
    "\n",
    "  # Stacking train data and test data to form single array named data\n",
    "  data = np.vstack([trainData, testData]) \n",
    "\n",
    "  # Vertical stacking labels of train and test set\n",
    "  labels = np.hstack([trainLabels, testLabels]) \n",
    "\n",
    "  # return a 2-tuple of the MNIST data and labels\n",
    "  return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f69a24-463a-4f8d-b518-512aba4d32ca",
   "metadata": {
    "id": "c7f69a24-463a-4f8d-b518-512aba4d32ca"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_az_dataset(datasetPath):\n",
    "\n",
    "  # List for storing data\n",
    "  data = []\n",
    "  \n",
    "  # List for storing labels\n",
    "  labels = []\n",
    "  \n",
    "  for row in open(datasetPath): #Openfile and start reading each row\n",
    "    #Split the row at every comma\n",
    "    row = row.split(\",\")\n",
    "    \n",
    "    #row[0] contains label\n",
    "    label = int(row[0])\n",
    "    \n",
    "    image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "    if image.size < 784:\n",
    "        image = np.pad(image, (0, 784 - image.size), 'constant')\n",
    "    image = image.reshape((28, 28))\n",
    "\n",
    "    \n",
    "    #append image to data\n",
    "    data.append(image)\n",
    "    \n",
    "    #append label to labels\n",
    "    labels.append(label)\n",
    "    \n",
    "  #Converting data to numpy array of type float32\n",
    "  data = np.array(data, dtype='float32')\n",
    "  \n",
    "  #Converting labels to type int\n",
    "  labels = np.array(labels, dtype=\"int\")\n",
    "  \n",
    "  return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42aac873-3b88-459f-b711-cb6cbd05122b",
   "metadata": {
    "id": "42aac873-3b88-459f-b711-cb6cbd05122b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372451, 28, 28)\n",
      "(70000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(digitsData, digitsLabels) = load_mnist_dataset()\n",
    "\n",
    "(azData, azLabels) = load_az_dataset('Data\\\\A_Z Handwritten Data\\\\A_Z Handwritten Data.csv')\n",
    "\n",
    "print(azData.shape)\n",
    "print(digitsData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae78d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# import necessary libraries\n",
    "import os\n",
    "\n",
    "# set the directory containing the dataset\n",
    "dataset_dir = \"Data/Englishm\"\n",
    "\n",
    "# initialize the data and labels lists\n",
    "en_data = []\n",
    "en_labels = []\n",
    "\n",
    "# loop over the directories inside the dataset directory\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    \n",
    "    # loop over the files in each directory\n",
    "    for file in files:\n",
    "        \n",
    "        # check if the file is an image\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\"):\n",
    "            \n",
    "            # construct the path to the image file\n",
    "            img_path = os.path.join(root, file)\n",
    "            \n",
    "            # load the image and resize it\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            img = cv2.resize(img, (28, 28))\n",
    "            # Convert to grayscale\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            # append the image data to the data list\n",
    "            en_data.append(img)\n",
    "            \n",
    "            # append the label to the labels list\n",
    "            label = os.path.basename(root)\n",
    "            en_labels.append(label)\n",
    "\n",
    "\n",
    "  #Converting data to numpy array of type float32\n",
    "en_data = np.array(en_data, dtype='float32')\n",
    "  #Converting labels to type int\n",
    "en_labels = np.array(en_labels)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder on your label data\n",
    "label_encoder.fit(en_labels)\n",
    "\n",
    "# Encode the labels\n",
    "en_labels = label_encoder.transform(en_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12839430-9923-41e2-8f64-d098903b01f3",
   "metadata": {
    "id": "12839430-9923-41e2-8f64-d098903b01f3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(445801, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "azLabels += 10\n",
    "en_labels+=37\n",
    "# stack the A-Z data and labels with the MNIST digits data and labels\n",
    "data = np.vstack([azData, digitsData,en_data])\n",
    "labels = np.hstack([azLabels, digitsLabels,en_labels])\n",
    "\n",
    "# Each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
    "# However, the architecture we're using is designed for 32x32 images,\n",
    "# So we need to resize them to 32x32\n",
    "\n",
    "data = [cv2.resize(image, (32, 32)) for image in data]\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "\n",
    "# add a channel dimension to every image in the dataset and scale the\n",
    "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
    "\n",
    "data = np.expand_dims(data, axis=-1)\n",
    "data /= 255.0\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e221a1c-715b-4337-94cc-d1c4ea95bfbf",
   "metadata": {
    "id": "0e221a1c-715b-4337-94cc-d1c4ea95bfbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 62\n",
      "Number of unique labels: 2\n",
      "(445801, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "unique_labels = np.unique(labels)\n",
    "num_classes = len(unique_labels)\n",
    "print(\"Number of unique labels:\", num_classes)\n",
    "le = LabelBinarizer()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "counts = labels.sum(axis=0)\n",
    "\n",
    "# account for skew in the labeled data\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "  classWeight[i] = classTotals.max() / classTotals[i]\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "num_classes = len(unique_labels)\n",
    "print(\"Number of unique labels:\", num_classes)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d70bb371-a6ca-4c90-b420-6b4a1487673d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d70bb371-a6ca-4c90-b420-6b4a1487673d",
    "outputId": "cfe3fd71-e8cc-4ae5-ffa4-526415f9fb48"
   },
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "rotation_range=10,\n",
    "zoom_range=0.05,\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "shear_range=0.15,\n",
    "horizontal_flip=False,\n",
    "fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79046f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import add\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "class ResNet:\n",
    "\t@staticmethod\n",
    "\tdef residual_module(data, K, stride, chanDim, red=False,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "\t\t# the shortcut branch of the ResNet module should be\n",
    "\t\t# initialize as the input (identity) data\n",
    "\t\tshortcut = data\n",
    "\n",
    "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
    "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(data)\n",
    "\t\tact1 = Activation(\"relu\")(bn1)\n",
    "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
    "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv1)\n",
    "\t\tact2 = Activation(\"relu\")(bn2)\n",
    "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "\t\t\tpadding=\"same\", use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "\t\t# the third block of the ResNet module is another set of 1x1\n",
    "\t\t# CONVs\n",
    "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv2)\n",
    "\t\tact3 = Activation(\"relu\")(bn3)\n",
    "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
    "\t\t# the shortcut\n",
    "\t\tif red:\n",
    "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# add together the shortcut and the final CONV\n",
    "\t\tx = add([conv3, shortcut])\n",
    "\n",
    "\t\t# return the addition as the output of the ResNet module\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, stages, filters,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "\t\t# initialize the input shape to be \"channels last\" and the\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# set the input and apply BN\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(inputs)\n",
    "\n",
    "\t\t# check if we are utilizing the CIFAR dataset\n",
    "\t\tif dataset == \"cifar\":\n",
    "\t\t\t# apply a single CONV layer\n",
    "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "\t\t# check to see if we are using the Tiny ImageNet dataset\n",
    "\t\telif dataset == \"tiny_imagenet\":\n",
    "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n",
    "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\t\tmomentum=bnMom)(x)\n",
    "\t\t\tx = Activation(\"relu\")(x)\n",
    "\t\t\tx = ZeroPadding2D((1, 1))(x)\n",
    "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "\t\t# loop over the number of stages\n",
    "\t\tfor i in range(0, len(stages)):\n",
    "\t\t\t# initialize the stride, then apply a residual module\n",
    "\t\t\t# used to reduce the spatial size of the input volume\n",
    "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
    "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t\t# loop over the number of layers in the stage\n",
    "\t\t\tfor j in range(0, stages[i] - 1):\n",
    "\t\t\t\t# apply a ResNet module\n",
    "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n",
    "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t# apply BN => ACT => POOL\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "\t\tx = Activation(\"softmax\")(x)\n",
    "\n",
    "\t\t# create the model\n",
    "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c63062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2786/2786 [==============================] - 475s 167ms/step - loss: 80.5427 - accuracy: 0.2993 - val_loss: 38.4186 - val_accuracy: 0.2194\n",
      "Epoch 2/50\n",
      "2786/2786 [==============================] - 460s 165ms/step - loss: 58.5194 - accuracy: 0.4578 - val_loss: 73.4784 - val_accuracy: 0.0074\n",
      "Epoch 3/50\n",
      "2786/2786 [==============================] - 459s 165ms/step - loss: 42.7228 - accuracy: 0.3887 - val_loss: 10.6267 - val_accuracy: 0.4894\n",
      "Epoch 4/50\n",
      "2786/2786 [==============================] - 458s 164ms/step - loss: 32.4146 - accuracy: 0.5625 - val_loss: 7.3150 - val_accuracy: 0.5655\n",
      "Epoch 5/50\n",
      "2786/2786 [==============================] - 458s 165ms/step - loss: 41.2253 - accuracy: 0.4678 - val_loss: 19.9663 - val_accuracy: 0.3684\n",
      "Epoch 6/50\n",
      "2786/2786 [==============================] - 459s 165ms/step - loss: 51.1354 - accuracy: 0.3241 - val_loss: 19.2461 - val_accuracy: 0.4450\n",
      "Epoch 7/50\n",
      "2786/2786 [==============================] - 459s 165ms/step - loss: 82.6118 - accuracy: 0.3708 - val_loss: 73.7701 - val_accuracy: 0.2169\n",
      "Epoch 8/50\n",
      "2786/2786 [==============================] - 457s 164ms/step - loss: 67.7352 - accuracy: 0.2908 - val_loss: 27.9859 - val_accuracy: 0.0343\n",
      "Epoch 9/50\n",
      "2786/2786 [==============================] - 459s 165ms/step - loss: 43.9878 - accuracy: 0.1733 - val_loss: 13.0900 - val_accuracy: 0.0966\n",
      "Epoch 10/50\n",
      "2786/2786 [==============================] - 458s 164ms/step - loss: 32.6128 - accuracy: 0.3733 - val_loss: 6.3224 - val_accuracy: 0.6130\n",
      "Epoch 11/50\n",
      "2786/2786 [==============================] - 458s 165ms/step - loss: 39.2569 - accuracy: 0.3886 - val_loss: 37.2322 - val_accuracy: 0.0755\n",
      "Epoch 12/50\n",
      "2786/2786 [==============================] - 458s 164ms/step - loss: 47.3756 - accuracy: 0.2573 - val_loss: 12.9991 - val_accuracy: 0.5645\n",
      "Epoch 13/50\n",
      "2786/2786 [==============================] - 458s 164ms/step - loss: 36.1165 - accuracy: 0.4536 - val_loss: 10.4175 - val_accuracy: 0.5663\n",
      "Epoch 14/50\n",
      "2786/2786 [==============================] - 458s 164ms/step - loss: 26.8465 - accuracy: 0.6803 - val_loss: 5.4359 - val_accuracy: 0.6902\n",
      "Epoch 15/50\n",
      "2786/2786 [==============================] - 461s 165ms/step - loss: 23.3875 - accuracy: 0.7441 - val_loss: 3.6849 - val_accuracy: 0.7988\n",
      "Epoch 16/50\n",
      "2786/2786 [==============================] - 469s 168ms/step - loss: 74.0558 - accuracy: 0.3914 - val_loss: 55.8457 - val_accuracy: 0.2492\n",
      "Epoch 17/50\n",
      "2786/2786 [==============================] - 462s 166ms/step - loss: 60.9778 - accuracy: 0.3865 - val_loss: 27.0644 - val_accuracy: 0.4316\n",
      "Epoch 18/50\n",
      "2786/2786 [==============================] - 466s 167ms/step - loss: 40.3148 - accuracy: 0.5299 - val_loss: 13.4945 - val_accuracy: 0.5308\n",
      "Epoch 19/50\n",
      "2786/2786 [==============================] - 461s 166ms/step - loss: 31.1127 - accuracy: 0.6211 - val_loss: 13.3336 - val_accuracy: 0.5638\n",
      "Epoch 20/50\n",
      "2786/2786 [==============================] - 462s 166ms/step - loss: 29.7363 - accuracy: 0.6441 - val_loss: 6.3548 - val_accuracy: 0.7310\n",
      "Epoch 21/50\n",
      "2786/2786 [==============================] - 459s 165ms/step - loss: 23.6500 - accuracy: 0.7351 - val_loss: 4.4339 - val_accuracy: 0.7589\n",
      "Epoch 22/50\n",
      "2786/2786 [==============================] - 459s 165ms/step - loss: 35.1937 - accuracy: 0.5718 - val_loss: 10.5696 - val_accuracy: 0.6327\n",
      "Epoch 23/50\n",
      "2786/2786 [==============================] - 458s 164ms/step - loss: 29.8959 - accuracy: 0.6200 - val_loss: 12.7369 - val_accuracy: 0.0234\n",
      "Epoch 24/50\n",
      "2786/2786 [==============================] - 461s 165ms/step - loss: 24.4443 - accuracy: 0.7230 - val_loss: 4.6800 - val_accuracy: 0.8030\n",
      "Epoch 25/50\n",
      "2786/2786 [==============================] - 462s 166ms/step - loss: 26.3862 - accuracy: 0.6518 - val_loss: 19.2615 - val_accuracy: 0.1535\n",
      "Epoch 26/50\n",
      "2786/2786 [==============================] - 459s 165ms/step - loss: 42.6695 - accuracy: 0.4543 - val_loss: 21.1466 - val_accuracy: 0.2773\n",
      "Epoch 27/50\n",
      "2786/2786 [==============================] - 458s 165ms/step - loss: 32.6743 - accuracy: 0.6327 - val_loss: 8.8699 - val_accuracy: 0.7461\n",
      "Epoch 28/50\n",
      "2786/2786 [==============================] - 459s 165ms/step - loss: 29.3919 - accuracy: 0.5655 - val_loss: 7.9213 - val_accuracy: 0.4911\n",
      "Epoch 29/50\n",
      "2786/2786 [==============================] - 460s 165ms/step - loss: 24.7111 - accuracy: 0.6649 - val_loss: 4.3638 - val_accuracy: 0.7446\n",
      "Epoch 30/50\n",
      "2786/2786 [==============================] - 464s 167ms/step - loss: 23.6742 - accuracy: 0.6599 - val_loss: 3.7996 - val_accuracy: 0.7018\n",
      "Epoch 31/50\n",
      "2786/2786 [==============================] - 461s 165ms/step - loss: 23.8375 - accuracy: 0.6515 - val_loss: 4.3450 - val_accuracy: 0.7253\n",
      "Epoch 32/50\n",
      "2786/2786 [==============================] - 461s 166ms/step - loss: 22.1579 - accuracy: 0.7220 - val_loss: 3.4169 - val_accuracy: 0.6418\n",
      "Epoch 33/50\n",
      "2786/2786 [==============================] - 461s 166ms/step - loss: 22.6205 - accuracy: 0.6743 - val_loss: 4.7697 - val_accuracy: 0.7047\n",
      "Epoch 34/50\n",
      "2786/2786 [==============================] - 462s 166ms/step - loss: 24.9015 - accuracy: 0.6448 - val_loss: 5.0303 - val_accuracy: 0.6886\n",
      "Epoch 35/50\n",
      "2786/2786 [==============================] - 461s 165ms/step - loss: 22.7052 - accuracy: 0.6881 - val_loss: 3.4131 - val_accuracy: 0.7587\n",
      "Epoch 36/50\n",
      "2786/2786 [==============================] - 462s 166ms/step - loss: 25.2470 - accuracy: 0.5968 - val_loss: 4.2738 - val_accuracy: 0.8474\n",
      "Epoch 37/50\n",
      "2786/2786 [==============================] - 464s 166ms/step - loss: 73.1618 - accuracy: 0.2937 - val_loss: 56.6280 - val_accuracy: 0.2419\n",
      "Epoch 38/50\n",
      "2786/2786 [==============================] - 465s 167ms/step - loss: 61.2982 - accuracy: 0.3770 - val_loss: 27.4468 - val_accuracy: 0.3949\n",
      "Epoch 39/50\n",
      "2786/2786 [==============================] - 463s 166ms/step - loss: 40.7323 - accuracy: 0.5669 - val_loss: 16.1247 - val_accuracy: 0.5291\n",
      "Epoch 40/50\n",
      "2786/2786 [==============================] - 464s 166ms/step - loss: 32.0615 - accuracy: 0.5758 - val_loss: 9.5794 - val_accuracy: 0.5904\n",
      "Epoch 41/50\n",
      "2786/2786 [==============================] - 464s 166ms/step - loss: 24.9160 - accuracy: 0.6842 - val_loss: 6.1420 - val_accuracy: 0.6282\n",
      "Epoch 42/50\n",
      "2786/2786 [==============================] - 464s 166ms/step - loss: 29.8764 - accuracy: 0.5013 - val_loss: 7.2516 - val_accuracy: 0.6191\n",
      "Epoch 43/50\n",
      "2786/2786 [==============================] - 464s 166ms/step - loss: 24.8690 - accuracy: 0.6428 - val_loss: 9.0780 - val_accuracy: 0.2033\n",
      "Epoch 44/50\n",
      "2786/2786 [==============================] - 464s 166ms/step - loss: 24.5153 - accuracy: 0.6346 - val_loss: 4.6195 - val_accuracy: 0.8106\n",
      "Epoch 45/50\n",
      "2786/2786 [==============================] - 462s 166ms/step - loss: 22.4719 - accuracy: 0.7085 - val_loss: 4.4912 - val_accuracy: 0.7346\n",
      "Epoch 46/50\n",
      "2786/2786 [==============================] - 464s 167ms/step - loss: 19.8756 - accuracy: 0.7794 - val_loss: 2.7463 - val_accuracy: 0.8269\n",
      "Epoch 47/50\n",
      "2786/2786 [==============================] - 464s 166ms/step - loss: 23.2075 - accuracy: 0.7521 - val_loss: 4.9959 - val_accuracy: 0.8094\n",
      "Epoch 48/50\n",
      "2786/2786 [==============================] - 464s 166ms/step - loss: 43.4748 - accuracy: 0.6405 - val_loss: 27.6101 - val_accuracy: 0.7686\n",
      "Epoch 49/50\n",
      "2786/2786 [==============================] - 464s 166ms/step - loss: 38.4345 - accuracy: 0.6918 - val_loss: 15.7811 - val_accuracy: 0.6661\n",
      "Epoch 50/50\n",
      "2786/2786 [==============================] - 465s 167ms/step - loss: 31.2865 - accuracy: 0.6274 - val_loss: 10.8659 - val_accuracy: 0.6233\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "EPOCHS = 50\n",
    "INIT_LR = 0.1\n",
    "BS=128\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=INIT_LR,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.96)\n",
    "opt = SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "\n",
    "# Build the model\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "    (64, 64, 128, 256), reg=0.0005)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "    trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY),\n",
    "    steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,\n",
    "    class_weight=classWeight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b52c1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('resnet.h5',save_format=\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testX.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfef7f1a-290a-4ba1-b8df-5683fd832e19",
   "metadata": {
    "id": "dfef7f1a-290a-4ba1-b8df-5683fd832e19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2787/2787 [==============================] - 27s 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.42      0.25      1386\n",
      "           1       0.85      0.95      0.89      1539\n",
      "           2       0.61      0.74      0.66      1402\n",
      "           3       0.77      0.66      0.71      1435\n",
      "           4       0.35      0.62      0.45      1362\n",
      "           5       0.14      0.79      0.24      1282\n",
      "           6       0.71      0.77      0.74      1370\n",
      "           7       0.78      0.83      0.80      1434\n",
      "           8       0.20      0.89      0.32      1396\n",
      "           9       0.84      0.58      0.69      1404\n",
      "           A       0.77      0.30      0.43      2725\n",
      "           B       0.47      0.29      0.36      1749\n",
      "           C       0.95      0.69      0.80      4648\n",
      "           D       0.43      0.76      0.55      2082\n",
      "           E       0.64      0.86      0.73      2278\n",
      "           F       0.89      0.84      0.87       218\n",
      "           G       0.43      0.80      0.56      1175\n",
      "           H       0.31      0.01      0.02      1446\n",
      "           I       0.60      0.90      0.72       221\n",
      "           J       0.56      0.73      0.63      1735\n",
      "           K       0.87      0.80      0.83      1159\n",
      "           L       0.87      0.95      0.91      2228\n",
      "           M       0.75      0.83      0.78      2429\n",
      "           N       0.73      0.31      0.43      3750\n",
      "           O       0.89      0.64      0.74     11550\n",
      "           P       0.96      0.82      0.89      3908\n",
      "           Q       0.42      0.81      0.55      1179\n",
      "           R       0.83      0.64      0.72      2354\n",
      "           S       0.99      0.04      0.07      9665\n",
      "           T       0.93      0.94      0.94      4511\n",
      "           U       0.85      0.77      0.81      5694\n",
      "           V       0.63      0.98      0.76       834\n",
      "           W       0.41      0.77      0.53      2237\n",
      "           X       0.90      0.91      0.91      1256\n",
      "           Y       0.87      0.82      0.84      2194\n",
      "           Z       0.85      0.78      0.82      1258\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     89161\n",
      "   macro avg       0.34      0.36      0.32     89161\n",
      "weighted avg       0.76      0.62      0.62     89161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lahbi\\anaconda3\\envs\\PFE\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2326: UserWarning: labels size, 72, does not match size of target_names, 36\n",
      "  warnings.warn(\n",
      "C:\\Users\\lahbi\\anaconda3\\envs\\PFE\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lahbi\\anaconda3\\envs\\PFE\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lahbi\\anaconda3\\envs\\PFE\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lahbi\\anaconda3\\envs\\PFE\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lahbi\\anaconda3\\envs\\PFE\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\lahbi\\anaconda3\\envs\\PFE\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#trainX, testX, trainY, testY = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "labelNames = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]\n",
    "\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), labels=np.arange(72), target_names=labelNames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc67533-2531-41b4-9c9d-5e145bb275fc",
   "metadata": {
    "id": "2bc67533-2531-41b4-9c9d-5e145bb275fc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "L\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "C\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "U\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "5\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "W\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "P\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "T\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "U\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "8\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "U\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "M\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "M\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "M\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "8\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "L\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "G\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "5\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "E\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "D\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "P\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "L\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "0\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "8\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "P\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "U\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "U\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "P\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "T\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "8\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "W\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "J\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "K\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "N\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "T\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "7\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "T\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "P\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "P\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "P\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "G\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "X\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "4\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "3\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Y\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "W\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "G\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"resnet.h5\")\n",
    "\n",
    "output = \"\"\n",
    "images = []\n",
    "test=\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(49,)):\n",
    "    probs = model.predict(testX[np.newaxis, i])\n",
    "    prediction = probs.argmax(axis=1)\n",
    "    print(labelNames[prediction[0]])\n",
    "    label = labelNames[prediction[0]]\n",
    "    output += label\n",
    "    image = (testX[i] * 255).astype(\"uint8\")\n",
    "    color = (0, 255, 0)\n",
    "    if prediction[0] != np.argmax(testY[i]):\n",
    "        color = (0, 0, 255)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
    "                color, 2)\n",
    "    images.append(image)\n",
    "\n",
    "\n",
    "montage = np.zeros((7 * 96, 7 * 96, 3), dtype=\"uint8\")\n",
    "\n",
    "for i in range(0, 49):\n",
    "    row = int(i / 7)\n",
    "    col = i % 7\n",
    "    montage[row * 96:(row + 1) * 96, col * 96:(col + 1) * 96] = images[i]\n",
    "\n",
    "cv2.imshow(\"Montage\", montage)\n",
    "cv2.waitKey(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de48987-e574-4df7-8994-bdbd205e37e0",
   "metadata": {
    "id": "2de48987-e574-4df7-8994-bdbd205e37e0"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tensorflow.keras.models import load_model\n",
    "import imutils\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a Tkinter root window (hidden)\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "# Ask the user to select an image file using a file dialog box\n",
    "file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png\")])\n",
    "\n",
    "\n",
    "model = load_model(\"resnet.h5\")\n",
    "labelNames = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# Read image\n",
    "image = cv2.imread(file_path)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Threshold image\n",
    "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "\n",
    "# Find contours in the image\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Sort contours left-to-right\n",
    "contours = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "\n",
    "output = \"\"\n",
    "for contour in contours:\n",
    "    # Get bounding box of contour\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Extract character image from thresholded image\n",
    "    roi = thresh[y:y+h, x:x+w]\n",
    "    \n",
    "    # Resize character image to 32x32\n",
    "    roi = cv2.resize(roi, (32, 32), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Add channel dimension to character image\n",
    "    roi = np.expand_dims(roi, axis=-1)\n",
    "    \n",
    "    # Normalize pixel values to range [0, 1]\n",
    "    roi = roi.astype(\"float\") / 255.0\n",
    "    \n",
    "    # Make prediction using OCR model\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        probs = model.predict(np.array([roi]))\n",
    "        prediction = probs.argmax(axis=1)\n",
    "        label = labelNames[prediction[0]]\n",
    "        output += label\n",
    "        cv2.putText(image, str(label[0]), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with recognized text\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# Print recognized text\n",
    "print(\"Recognized text:\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87458b5-f7dc-4e86-85ca-0b4b1237bfe9",
   "metadata": {
    "id": "e87458b5-f7dc-4e86-85ca-0b4b1237bfe9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31134eaf-ef16-4f48-8d11-76a38d45f12e",
   "metadata": {
    "id": "31134eaf-ef16-4f48-8d11-76a38d45f12e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac493a-b3ad-4185-884f-752eb9a382af",
   "metadata": {
    "id": "4fac493a-b3ad-4185-884f-752eb9a382af"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bf0e0-fb5a-47a4-a4e6-06a22e148c2f",
   "metadata": {
    "id": "a55bf0e0-fb5a-47a4-a4e6-06a22e148c2f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160aede-c0ae-469f-8092-f245cf0965d2",
   "metadata": {
    "id": "6160aede-c0ae-469f-8092-f245cf0965d2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdc2f9-1129-4325-9eb9-f3111212a84c",
   "metadata": {
    "id": "a8fdc2f9-1129-4325-9eb9-f3111212a84c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a98ef-961c-4242-b215-daa48c9280b9",
   "metadata": {
    "id": "ce9a98ef-961c-4242-b215-daa48c9280b9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff1176-b054-400a-a085-664c11a3b09f",
   "metadata": {
    "id": "57ff1176-b054-400a-a085-664c11a3b09f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145029dd-0fa4-43cf-b744-3f97b2f0ccb4",
   "metadata": {
    "id": "145029dd-0fa4-43cf-b744-3f97b2f0ccb4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e6273-c34c-44bf-9e9f-d99a40f6e2f0",
   "metadata": {
    "id": "d00e6273-c34c-44bf-9e9f-d99a40f6e2f0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b90c1-31f5-42e6-8566-188d486ba76a",
   "metadata": {
    "id": "629b90c1-31f5-42e6-8566-188d486ba76a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6c854-4f73-4dcb-a94c-c1c5225c17a0",
   "metadata": {
    "id": "d4a6c854-4f73-4dcb-a94c-c1c5225c17a0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf935028-09bf-4ab8-b833-88f2b43b2f6f",
   "metadata": {
    "id": "cf935028-09bf-4ab8-b833-88f2b43b2f6f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc5375-6145-4ea6-95d8-011b61d01817",
   "metadata": {
    "id": "7adc5375-6145-4ea6-95d8-011b61d01817"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e601f0-8777-4bde-b8a6-588fe8204345",
   "metadata": {
    "id": "60e601f0-8777-4bde-b8a6-588fe8204345"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457a408-5a46-4ccc-8bcd-46bf15480853",
   "metadata": {
    "id": "9457a408-5a46-4ccc-8bcd-46bf15480853"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cab8e-871c-422f-a19b-ec397c3937c8",
   "metadata": {
    "id": "269cab8e-871c-422f-a19b-ec397c3937c8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5da6e-d8c6-474a-b434-ee6b4b9f1a41",
   "metadata": {
    "id": "59b5da6e-d8c6-474a-b434-ee6b4b9f1a41"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280b3b7-f69a-42ad-a5a8-51e6c05b6fc2",
   "metadata": {
    "id": "2280b3b7-f69a-42ad-a5a8-51e6c05b6fc2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52881bb-97b5-427f-a767-4c2fed49e0f1",
   "metadata": {
    "id": "a52881bb-97b5-427f-a767-4c2fed49e0f1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46dff8e-f14f-4db2-ba66-8ab38d351654",
   "metadata": {
    "id": "f46dff8e-f14f-4db2-ba66-8ab38d351654"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645b7b2-dec4-4a7d-8bb1-7c972c841d4b",
   "metadata": {
    "id": "9645b7b2-dec4-4a7d-8bb1-7c972c841d4b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637b08d-c938-49ea-b1d0-93674facb436",
   "metadata": {
    "id": "1637b08d-c938-49ea-b1d0-93674facb436"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c6706-e218-4ae8-b29d-110b1dbdba97",
   "metadata": {
    "id": "041c6706-e218-4ae8-b29d-110b1dbdba97"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022e09c-507a-4b0b-b2fa-f5b9be42582f",
   "metadata": {
    "id": "9022e09c-507a-4b0b-b2fa-f5b9be42582f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225a8f5-c692-43cb-ae32-d9a3c68e6957",
   "metadata": {
    "id": "0225a8f5-c692-43cb-ae32-d9a3c68e6957"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca37699-be20-47b2-93f4-0f374f64c4ef",
   "metadata": {
    "id": "aca37699-be20-47b2-93f4-0f374f64c4ef"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa244656-a018-41e0-b1c0-ddd6bd993e21",
   "metadata": {
    "id": "aa244656-a018-41e0-b1c0-ddd6bd993e21"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a81234-9c86-4648-b6c0-88df8b43968f",
   "metadata": {
    "id": "f1a81234-9c86-4648-b6c0-88df8b43968f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279c3ca-f4de-4ed5-a6ea-3320d8d23ff2",
   "metadata": {
    "id": "4279c3ca-f4de-4ed5-a6ea-3320d8d23ff2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109695d2-3617-442a-ac17-e70127c6658e",
   "metadata": {
    "id": "109695d2-3617-442a-ac17-e70127c6658e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524ab62-cbd7-40eb-b34a-b171cd0881ac",
   "metadata": {
    "id": "7524ab62-cbd7-40eb-b34a-b171cd0881ac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989834a-dfd4-4c7f-97eb-08f6adc8f5f3",
   "metadata": {
    "id": "e989834a-dfd4-4c7f-97eb-08f6adc8f5f3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a3dd6-d46f-4d77-be1c-d7076744dff1",
   "metadata": {
    "id": "900a3dd6-d46f-4d77-be1c-d7076744dff1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78663495-dd79-4b86-8873-4a4ff2fe022e",
   "metadata": {
    "id": "78663495-dd79-4b86-8873-4a4ff2fe022e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41af652-18fa-4f1a-b13a-3271da0b8fc7",
   "metadata": {
    "id": "b41af652-18fa-4f1a-b13a-3271da0b8fc7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
